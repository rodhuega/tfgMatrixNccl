\section{Introduction and notation}
In recent years, the study of matrix functions has been the subject of increasing focus due to its usefulness in various areas of
science and engineering, providing new and interesting problems to those already existing and already well-known. Of all matrix functions,
it is certainly the exponential matrix which attracts much of the attention because of its connection with linear first
order differential systems

$$
\left.
\begin{array}{rcl}
Y'(t)&=& AY(t) \\
Y(0)&=& Y_{0} \\
\end{array}\right\} \ , \ A \in \mathbb{C}^{r \times r}
$$
whose solution is given by $Y(t)=e^{A t} Y_{0}$. The hyperbolic matrix functions are applied in the study of the communicability analysis in complex
networks \cite{estrada2008communicability,estrada2005spectral,estrada2016network} and also in the solution of coupled hyperbolic systems
of partial differential equations \cite{jodar2003constructive}. In particular, the trigonometric matrix functions sine and cosine prove especially useful
 in solving systems of second-order linear differential equations of the form:
$$
\left.
\begin{array}{rcl}
\displaystyle \frac{d^2}{dt^2}Y(t)+A^2 Y(t)&=&0 \\
Y(0)&=& Y_{0} \\
Y'(0)&=& Y'_{0} \\
\end{array}\right\} \ , \ A \in \mathbb{C}^{r \times r}
$$
whose solution, if matrix $A$ is non-singular, is given by
$$
Y(t)=\cos{(At)} Y_0+ A^{-1}\sin{(At)}Y'_0.
$$
Due to the relationship $\displaystyle \sin{(A)}=\cos{\left(A + \frac{\pi}{2} I \right)}$, where $I$ is the identity matrix of $\mathbb{C}^{r \times r}$, the
matrix sine can be calculated using the same methods as for the matrix cosine. Usually, research concentrated on approximate calculations of the matrix cosine, 
developing several efficient state-of-the-art algorithms to approximate it. These
methods and algorithms can be found in references \cite{Serb80, dehghan2010computing, High08, alonso2018computing}. Other algorithms, for normal and
nonnegative matrices, based on approximations $L_{\infty}$ also have been presented in Ref. \cite{tsitouras2014bounds}. \\

Among the methods proposed to approximate the matrix cosine, two stand out fundamentally: those based on polynomial approximations (in general based on the
developments of the matrix cosine in Taylor or Hermite series, see \cite{sastre2017two, sastre2019fast, defez2019efficient}) or those based on rational
approximations (i.e. Pad\'e approximation, see \cite{tsitouras2014bounds, Serb79, Serb80, AlHR15}). Normally, polynomial methods are more efficient
in terms of accuracy (although somewhat more computationally expensive) than rational ones.\\

On the other hand, Bernoulli polynomials (and numbers) introduced by Jacob Bernoulli  (1654--1705) in the 18th century, are widely used in various areas of mathematics, both pure and applied, see for example \cite{kouba2013lecture} and references therein.\\

In this paper, we will define Bernoulli matrix polynomials and present a new series expansion of the matrix sine and cosine functions in terms
of these matrix polynomials. Then, we will check that this series expansion will provide a new efficient method to approximate the
matrix cosine.\\

The organization of the paper is as follows: In section \ref{section2}, we will obtain two serial expansions of the matrix cosine in terms of the Bernoulli
matrix polynomials. In section \ref{section4}, we will perform different numerical tests. Conclusions are given in section XXX.\\

Throughout this paper, a polynomial of degree $m$ is given by an expression of the form $P_m(x)=a_{m} x^m+a_{m-1}x^{m-1}+\cdots+a_{1}x+a_{0}$, where $x$ is
the variable (real or complex) and the coefficients $a_j, 0\leq j \leq m$, are complex numbers. Moreover, we can define the matrix polynomial $P_m(B)$ for
$B \in \mathbb{C}^{r \times r}$  as the expression $P_m(B)=a_{m} B^m+a_{m-1}B^{m-1}+\cdots+a_{1}B+a_{0}I$.  As usual, the matrix norm $\left\|\cdots \right\|$
denotes any subordinate matrix norm; in particular $\left\| \cdots \right\|_{1}$ is the usual $1$-norm.

\section{On Bernoulli matrix polynomials}\label{section2}
The sequence of Bernoulli polynomials (denoted by $\left\{B_n(x)\right\}_{n \geq 0}$) and the Bernoulli numbers, $B_n=B_{n}(0)$, appear in important applications for different areas of mathematics, from number theory to classical analysis. For example, they are used for
representing the remainder term of the composite Euler-MacLaurin quadrature rule. They also appear in the Taylor expansion in the neighborhood of the origin of circular and
hyperbolic tangent and co-tangent functions. Moreover, this sequence expresses the exact value of $\zeta(2p)$, with $p$ a positive integer, where $\zeta(z)=\displaystyle \sum_{k \geq 1} \frac{1}{k^{z}}$ is the
well-known Riemann's zeta function. They were first studied by Jacob Bernoulli before 1705 in relation with the computation of sums of
powers of $m$ consecutive integers $\displaystyle S_r(m)=\sum_{k=1}^{m} k^r$, where $r$ and $m$ are two given positive integers. The usual way to define these Bernoulli
polynomials, see \cite[p.~588]{olver2010nist} , is as the coefficients of the Taylor expansion of the following generating function
\begin{equation}\label{Bernoulli1}
g(x, t)= \frac{t e^{tx}}{e^t-1}=\sum_{n \geq 0} \frac{B_n(x)}{n!}t^n  \ , \ |t|<2\pi,
\end{equation}
where $g(x, t)$ is a holomorphic function in $\mathbb{C}$ for the variable $t$ (function $g(x, t)$ has an avoidable singularity in $t=0$). Polynomials $B_n(x)$ have the explicit expression
\begin{equation}\label{Bernoulli2}
B_n(x)=\sum_{k=0}^{n} {n \choose k} B_k x^{n-k},
\end{equation}
where the Bernoulli numbers $B_n$ are defined by the recurrence relation
\begin{equation}\label{Bernoulli3}
B_0=1, \displaystyle  B_{k}= -\sum_{i=0}^{k-1} {k \choose i} \frac{B_i}{k+1-i}, k \geq 1.
\end{equation}
Note that all Bernoulli numbers with impair index vanish, except for $B_{1}=-1/2$. For a matrix $A \in \mathbb{C}^{r \times r}$, we define the $m$-th Bernoulli matrix polynomial by the expression
\begin{equation}\label{Bernoulli4}
B_m(A)=\sum_{k=0}^{m} {m \choose k} B_k A^{m-k}.
\end{equation}
The series expansion of the exponential matrix function $e^{At}$ given by
\begin{equation}\label{Bernoulli5}
e^{At} = \left(\frac{e^t-1}{t}\right)\sum_{n \geq 0} \frac{ B_n(A) t^{n}}{n!} \ , \ 0<|t|<2\pi,
\end{equation}
was demonstrated in Ref. \cite{defez2019}. An efficient method based on formula (\ref{Bernoulli5}) for approximating the exponential matrix has been presented and developed in Ref. \cite{defez2019}.\\

Taking $t=1$ in (\ref{Bernoulli5}) and from the definition of the matrix sine and cosine, it is easy to derive the following expressions
\begin{equation}\label{Bernoulli6}
\left.\begin{array}{rcl}
\cos{(A)} &=&\displaystyle  \left( \cos{(1)}-1\right)\sum_{n \geq 0} \frac{(-1)^n B_{2n+1}(A)}{(2n+1)!}+ \sin{(1)}\sum_{n \geq 0} \frac{(-1)^n B_{2n}(A)}{(2n)!}, \\
\\
\sin{(A)} &=& \displaystyle   \sin{(1)}\sum_{n \geq 0} \frac{ (-1)^n B_{2n+1}(A)}{(2n+1)!}-\left(\cos{(1)}-1\right)\sum_{n \geq 0} \frac{ (-1)^n B_{2n}(A)}{(2n)!}.
\end{array} \right\}
\end{equation}
Replacing in (\ref{Bernoulli5}) the value $t$ for $it$ and $-it$ respectively and taking the arithmetic mean, we can also obtain the result
\begin{equation}\label{Bernoulli7}
\sum_{n \geq 0} \frac{(-1)^n B_{2n}(A)}{(2n)!}t^{2n} =\frac{t}{2 \sin{\left( \frac{t}{2} \right)}}\left(\cos{\left(t A- \frac{t}{2}I \right)}  \right)  \ , \ 0<|t|<2\pi.
\end{equation}
Taking $t=2$ in (\ref{Bernoulli7}), it follows that
\begin{equation}\label{Bernoulli8}
\cos{(A)} = \sin{(1)}\sum_{n \geq 0} \frac{(-1)^n 2^{2n} B_{2n}\left(\frac{A+I}{2}\right)}{(2n)!},
\end{equation}
Note that in formula (\ref{Bernoulli8}) only Bernoulli's polynomials with even index appear, similar to what happens when considering cosine
series expansions using Taylor or Hermite polynomials \cite{sastre2017two,defez2019efficient}.

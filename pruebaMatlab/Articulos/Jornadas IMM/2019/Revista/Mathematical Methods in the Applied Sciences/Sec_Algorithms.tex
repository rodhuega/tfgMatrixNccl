\section{Algorithms} \label{section3}
If we truncate Expressions \eqref{Bernoulli6} or \eqref{Bernoulli8} of cosine function, then we obtain the following approximation:
\begin{equation}
\label{Pm}
{P_m}(A) = \sum\limits_{i = 0}^m {{p_i^{(m)}}{A^i}},
\end{equation}where the coefficients ${p_i^{(m)}}$ depend on  
 the integer $m$.  These coefficients  converge to the coefficients of the Taylor series when $m$ tends to $\infty$. In Section \ref{section4} three approximations have been considered: two approximations based on Expression  \eqref{Bernoulli6}, an approximation in which all the coefficients are considered and another in which only the coefficients of the even order terms have been considered, and one approximation based on Expression \eqref{Bernoulli8}, where all the coefficients are considered. According to the approaches considered,  two algorithms  has been developed.

\begin{algorithm}[!ht]
\caption{Given a matrix $A \in {\mathbb{C}^{n \times n}}$, this algorithm computes $C=\cos(A)$ by Bernouilli series \eqref{Bernoulli6} or \eqref{Bernoulli8}, where all coefficients have been considered.}
\label{Alg_cos1}
\begin{algorithmic} [1]
\State Select adequate values of $m_k$ and $s \in \mathbb{N} \cup \left\{ 0 \right\}$ \Comment Phase I
\State $A=2^{-s}A$
\State $C=P_{m_k}(A)$ \Comment Phase II: Compute Bernouilli approximation \eqref{Bernoulli6} or \eqref{Bernoulli8} 
 \For {$i=1:s$} \Comment Phase III: Recovering $\cos(A)$
    \State $C=2C^{2}-I$
 \EndFor
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[!ht]
\caption{Given a matrix $A \in {\mathbb{C}^{n \times n}}$, this algorithm computes $C=\cos(A)$ by Bernouilli series \eqref{Bernoulli6}, where only the coefficients of the even order terms have been considered.}
\label{Alg_cos2}
\begin{algorithmic} [1]
\State Select adequate values of $m_k$ and $s \in \mathbb{N} \cup \left\{ 0 \right\}$ \Comment Phase I
\State $A=4^{-s}A^2$
\State $C=P_{m_k}(A)$ \Comment Phase II: Compute Bernouilli approximation \eqref{Bernoulli6}
 \For {$i=1:s$} \Comment Phase III: Recovering $\cos(A)$
    \State $C=2C^{2}-I$
 \EndFor
\end{algorithmic}
\end{algorithm}

In Phase I the integers $m$ and $s$ are
computed so that the Bernouilli approximation of the scaled matrix is computed accurately and efficiently. 
There are several methods can be applied to compute efficiently $C=P_{m_k}(A)$ in Phase II, see ~\cite{PaSt73} and \cite{sastre2018efficient}. In our implementations we used the first one based on  the Paterson-Stockmeyer's
method.
In this method, the integer $m_k$ is chosen from the set
\begin{displaymath}
\label{m_k} \mathbb{M}=\left\{2,4, 6, 9,12,16,20,25,30,36,42,\dots\right\},
\end{displaymath}
where  only the  powers $A^i$, $2\leq i\leq
q$, are computed, being $q = \left\lceil {\sqrt
{{m_k}} } \right\rceil $ or $q=\lfloor \sqrt{m_{k}}\rfloor$ an
integer divisor of $m_{k}$. Then, $C=P_{m_k}(A)$ can
be computed efficiently as
{\setlength\arraycolsep{2pt}{%\small
\begin{align}
\label{PS}
P_{m_k}(A) & =  \\ \nonumber ((( p_{m_k}A^q & +  p_{m_k-1}A^{q-1}
+ p_{m_k-2}A^{q-2}   + \dots + p_{m_k-q+1}A  + p_{m_k-q} I ) A^q \\
\nonumber
               & +  p_{m_k-q-1}A^{q-1} + p_{m_k-q-2}A^{q-2} + \dots + p_{m_k-2q+1}A + p_{m_k-2q} I ) A^q \\ \nonumber
               & +  p_{m_k-2q-1}A^{q-1} + p_{m_k-2q-2}A^{q-2} + \dots + p_{m_k-3q+1}A + p_{m_k-3q} I ) A^q \\ \nonumber
               & \dots  \\ \nonumber
               & +  p_{q-1}A^{q-1} + p_{q-2}A^{q-2} + \dots + p_{1}A + p_{0} I.
\end{align}}}
The computational cost in terms of matrix products of~\eqref{PS} is $\Pi {m_k} = k$.

The computation of the scaling factor $s$ and the order of Bernouilli approximation $m_{k}$  are based on the errors made truncating a series. One first idea is to exploit the sequence $\left\{ {{{\left\| {{A^k}} \right\|}^{1/k}}} \right\}$.
As $\rho (A) \leqslant {\left\| {{A^k}} \right\|^{1/k}} \leqslant \left\| A \right\|$, with $k \geqslant 0$, where $\rho$ is the spectral radius of the matrix $A$, then 
\[\mathop {\lim }\limits_{k \to \infty } ||{A^k}|{|^{1/k}} = \rho (A).\]

Theorem 1.1 from \cite{High09}
shows that if ${h_l} = \sum\limits_{k\geqslant l}^{} {{c_k}{x^k}}$ is a power series with a radius convergence $w$ and ${{\tilde h}_l} = \sum\limits_{k\geqslant l}^{} {\left| {{c_k}} \right|{x^k}}$ , then for any $A \in {\mathbb{C}^{n \times n}}$ 
$$\left\| {{h_l}(A)} \right\| \leqslant {{\tilde h}_l}(||{A^t}|{|^{1/t}}),$$
where $||{A^t}|{|^{1/t}} = \max \left\{ {||{A^k}|{|^{1/k}}:\,\,k \geqslant l,\,\,\,{c_l} \ne 0} \right\}$.

Using the same notation, Theorem 1.1 from \cite{sastre2013exp} shows that if $p_0$ is the multiple of $t$ such that $$l \leqslant {t_0} \leqslant l + p - 1,$$
if $a_{k}$ is an upper bound for $||A^{k}||$ $(||A^{k}||\leq
a_{k})$, $p \in \mathbb{N}$, $1\leq p \leq l$, $p_0\in \mathbb{N}$
is the multiple of $p$ with $l\leq p_0\leq l+p-1$, and
\begin{equation}\label{alphap}
\alpha_{p}=\max\{a_{k}^{\frac{1}{k}}:k=p,l,l+1,l+2,\ldots,p_0-1,p_0+1,p_0+2,\ldots,l+p-1\},
\end{equation}then
%\begin{equation}\label{hlbound}
$||h_{l}(A)||\leq \tilde{h}_{l}(\alpha_{p}).$
%\end{equation}\]

If we apply Theorem 1.1 from \cite{sastre2013exp} for $a_k=||A^{k}||$ and we consider $p=l$, then
$$||h_{l}(A)||\leq \tilde{h}_{l}(\alpha_{l}),$$
where $\alpha_{l}=\max\{||A^{k}||^{\frac{1}{k}}:k=l,l+1,l+2,\ldots,2l-1\}$. This result reduces the values of $||A^{k}||^{\frac{1}{k}}$  given in Theorem 1.1 from \cite{High09}.
In this paper, we propose use the following approximation:
\begin{equation}
\label{Estimation}
\alpha _m \approx ||A^{m}||^{1/m}.
\end{equation}
Using these results, the calculations of $m$ and $s$ from PHASE I of Algorithms \ref{Alg_cos1} and \ref{Alg_cos2} are based on the   relative backward  of
approximating $\cos(A)$ by the approximation \eqref{Pm}.  This error is defined as the matrix $\Delta A$ such that $\cos(A+\Delta A)=P_{m}(A)$. Below, we bound the relative backward error as follows:    
\begin{displaymath}\label{Eb}
E_b=\frac{{\left\| {\Delta A} \right\|}}{{\left\| A \right\|}} =
\frac{{\left\| {\sum\limits_{i \ge0} {{c^{(m)}_i}{A^{^{i+1}}}} }
\right\|}}{{\left\| A \right\|}} \backsimeq\ \frac{{\left\| {\sum\limits_{i \ge m+1} {{c^{(m)}_i}{A^{^{i+1}}}} }
\right\|}}{{\left\| A \right\|}} \le  \left\| {\sum\limits_{i \ge m}
{{c^{(m)}_i}{A^{i}}} } \right\|.
\end{displaymath}
If we define ${h_{m}}(x) = \sum\limits_{i \geqslant m} {{c^{(m)}_i}{x^{i}}} {\text{ }}$ and ${\tilde h_{m}}(x) = \sum\limits_{i \ge
m} {\left| {{c^{(m)}_i}} \right|{x^{i}}} {\rm{ }}$, then
\begin{equation}
E_b \le \left\| {{h_{m}}(A)} \right\| \le {{\tilde h}_{m}}({{\alpha}
_{m}}).
\end{equation}Let $\Theta_{m}$ be
\begin{equation} {\Theta_{m}} = \max \left\{
{{\theta\geq0}:\,\,\sum\limits_{i \ge m} {\left| {{c^{(m)}_i}}
\right|{\theta}^{i}}  \le u} \right\},
\end{equation}
where $u$ is the unit roundoff in IEEE double precision arithmetic ($u=2^{-53}$). For computing $ {\Theta}_{m}$, the MATLAB
Symbolic Math Toolbox has been used.  

On the other hand, if $\alpha_m<\Theta_m$, then
\begin{equation}
{E_b} \leqslant \left\| {{h_m}(A)} \right\| \leqslant {\tilde h_m}(\alpha _m) \leqslant {\tilde h_m}({\Theta _m}) \leqslant u.
\end{equation}
Hence, if $\alpha_m<\Theta_m$, then $E_b\le u$; if not, we would find a value of $s$ such that $2^{-s}\alpha_m<\Theta_m$ (in this case we   compute $P_{m_k}(2^{-s}A)$ in step 3 of Algorithm \ref{Alg_cos1}) or a value of $s$ such that $4^{-s}\alpha_m<\Theta_m$ (in this case we  compute $P_{m_k}(4^{-s}A)$ in step 3 of Algorithm \ref{Alg_cos2}). 

The norms $||A^{m}||^{1/m}$ can be computed approximately using  matrix powers
previously computed  based on the estimation of norms of matrix powers  block 1-norm estimation algorithm
from~\cite{High88}. Taking  into account
that analysis, Algorithms \ref{ms-1}, \ref{ms-2} and \ref{ms-3}  have been developed.  

Algorithms \ref{ms-1} and \ref{ms-2} initially check whether there is a value $m_i$,  $m_{\min}\le m_{i}\le m_{\max}$,
such that $\alpha_{m_{i}} \leq \Theta_{m_{i}}$, computing the necessary powers of matrix $A$  to obtain $P_{i}(A)$ from \eqref{PS} as $i$ increases. The values  of  $m_{\min}$ and $ m_{\max}$ can be  varied in the developed implementations.
 If there is such value $m_i$, then we choose the lower
order $m=m_i$ such that $\alpha_{m_{i}} \leq \Theta_{m_{i}}$ and  $s=0$.
Otherwise, we choose $m=m_{\max}$ and
\begin{equation*}
s = \max \left\{ {0,\left\lceil f_{s}\log \left(
{\frac{{{\alpha_{m_{\max}}}}}{{\Theta_{_{m_{\max}}})}}} \right)
\right\rceil } \right\},
\end{equation*}%Algoritmo EstNormaConPotencias 
where $fs=1$, if Algorithm \ref{Alg_cos1} is used, or  $fs=0.5,$ if Algorithm \ref{Alg_cos2} is used. In Algorithm \ref{ms-1} the lines 20-29 test if it is possible to decrease the above values $s$ and $m$ (the initial value of $m$ is $m_{\max}$), so that
\[{\tilde h_m}(2^{-s}\alpha _m) \leqslant {\tilde h_m}({\Theta _m}) \leqslant u \]
is fulfilled, when Algorithm \ref{Alg_cos1} is used, or \[{\tilde h_m}(4^{-s}\alpha _m) \leqslant {\tilde h_m}({\Theta _m}) \leqslant u\]
 it is fulfilled, when Algorithm \ref{Alg_cos2}.
In those cases, those new values are considered.

In Algorithm \ref{ms-2} the lines 19-24 test if it is possible to decrease the value $s$ so that
  $$|p_{m_{\max}}|a_{m_{\max}}2^{(1-s)m_{\max}}<u,$$
if Algorithm \ref{Alg_cos1} is used, or
$$|p_{m_{\max}}|a_{m_{\max}}4^{(1-s)m_{\max}}<u,$$
if Algorithm \ref{Alg_cos2} is used, where $p_{m_{\max}}$ is the  coefficient of the term of order $m_{\max}$ of Bernouilli series. 

Unlike algorithms \ref{ms-1} and \ref{ms-2}, Algorithm \ref{ms-3} does not compute the powers of matrix $A$, so the estimate \eqref{Estimation} is obtained only from $A$. Algorithm \ref{ms-3} computes $\alpha_m$ such that
\[\left| {\frac{{\alpha_{m_{i}} - \alpha_{m_{i-1}}}}{{\alpha_{m_{i}}}}} \right| < tol,\]
where $tol$ is a small prefixed value (see Lines 3-11).
Then Algorithm \ref{ms-3} check if  $ m_{i}\le m_{\max}$ such that $\alpha_{_{m_{i}}} \leq \Theta_{m_{i}}$. In this case,  we choose the lower
order $m=m_i$ such that $\alpha_{m_{i}} \leq \Theta_{m_{i}}$ and  $s=0$.
Otherwise, we choose $m=m_{\max}$ and
\begin{equation*}
s = \max \left\{ {0,\left\lceil f_s\log \left(
{\frac{{{\alpha_{m_{\max}}}}}{{\Theta_{m_{\max}}})}} \right)
\right\rceil } \right\}.
\end{equation*} 
Next, in the lines 23-32 of Algorithm \ref{ms-3}  is checked if it is possible to decrease the above values $s$ and $m$ so that
\[{\tilde h_m}(2^{-s}\alpha _m) \leqslant {\tilde h_m}({\Theta _m}) \leqslant u \]
is fulfilled, when Algorithm \ref{Alg_cos1} is used, or \[{\tilde h_m}(4^{-s}\alpha _m) \leqslant {\tilde h_m}({\Theta _m}) \leqslant u\]
 it is fulfilled, when Algorithm \ref{Alg_cos2}.
In those cases, those new values are considered.

  \begin{algorithm}[!ht]
\caption{Given a matrix $A \in {\mathbb{C}^{n \times n}}$, a minimum order $m_{\min}\in\mathbb{M}$ and a maximum order $m_{\max}\in\mathbb{M}$, this algorithm calculates an order $m_i\in\mathbb{M}$, $m_{\min}\le m\le m_{\max}$, a factor $s$ and several powers of $A$ for computing $C$ in PHASE II.}
\label{ms-1}
\begin{algorithmic} [1]
\State $A_1=A$; $i=\min$; $f$=0
\While {$f=0$ and $i\le \max$}
    \State $v=\sqrt {m_{i}}$
    \State $j=\left\lceil v\right\rceil$
    \If {$v>j$}
        \State $A_j=A_{j-1}A$ 
    \EndIf
    \State ${\alpha_{m_{i}}} \approx {\left\| {{A^{m_i}}} \right\|^{1/m_i}}$ from $A_j$  \Comment based on Algorithm 1  from~\cite{High88} 
    \If {$\alpha_{m_{i}}<\Theta_{m_{i}}$  }
        \State $f=1$
    \Else
         \State $i=i+1$
    \EndIf 
\EndWhile
\If {$f=1$}
    \State $s=0$
\Else
    \State $s = \left\lceil {\max \left( {0,{{f_{s}\log }_2}({\alpha_{m_{\max}}}/{\Theta_{m_{\max}}})} \right)} \right\rceil $ or  
    \State $j=i_{\min}$
    \State $f=0$
    \While {$f=0$}
       \State $j=j-1$
       \State $s_1 = \left\lceil {\max \left( {0,{{f_{s}\log }_2}({\alpha_{m_{j}}}/{\Theta_{m_{j}}})} \right)} \right\rceil $
       \If {$s\ge s_1$ and $j\geq i_{\min}$}
            \State $s=s_1$
            \State $i=j$
       \Else 
            \State $f=1$  
       \EndIf 
    \EndWhile   
\EndIf
\State $m=m_i$    
\end{algorithmic}
\end{algorithm}

%Algoritmo EstNormaConPotenciasNuevo 
\begin{algorithm}[!ht]
\caption{Given a matrix $A \in {\mathbb{C}^{n \times n}}$, a minimum order $m_{\min}\in\mathbb{M}$ and a maximum order $m_{\max}\in\mathbb{M}$, this algorithm calculates an order $m\in\mathbb{M}$, $m_{\min}\le m\le m_{\max}$, a scaling factor $s$  and the necessary powers of $A$ for computing $C$ in PHASE II.}
\label{ms-2}
\begin{algorithmic} [1]
\State $A_1=A$; $i=\min$; $f$=0
\While {$f=0$ and $i\le \max$} 
    \State $v=\sqrt {m_{i}}$
    \State $j=\left\lceil v\right\rceil$
    \If {$v>j$}
        \State $A_j=A_{j-1}A$ 
    \EndIf
    \State Compute ${a_{m_{i}}} \approx {\left\| {{A^{m_i}}} \right\|}$ from $A^{j}$  \Comment based on Algorithm 1  from~\cite{High88} 
    \State ${\alpha _{{m_i}}} = \sqrt[{{m_i}}]{{{a_{{m_i}}}}}$
    \If {$\alpha_{mi}<\Theta_{m_{i}}$}
        \State $f=1$ 
    \Else
         \State $i=i+1$
    \EndIf
\EndWhile
\If {$f=1$}
    \State $s=0$
\Else
    \State $i=i_{\max}$
    \State $s = \left\lceil {\max \left( {0,{{f_s\log }_2}({\alpha_{m_{i}}}/{\Theta_{mi}})} \right)} \right\rceil $ 
\If {$|p_{m_{i}}|a_{m_{i}}r^{(-s+1)m_{i}}<u$} \Comment $r=2$ (Algorithm \ref{Alg_cos1})
 \State \Comment  $r=4$ (Algorithm \ref{Alg_cos2}) 
    \State $s=s-1$
    \If {$|p_{m_{i}}|a_{m_{i}}r^{(-s+1)m_{i}}<u$}
       \State $s=s-1$
    \EndIf 
\EndIf
\EndIf
\State $m=m_i$    
\end{algorithmic}
\end{algorithm}

%Algoritmo EstNormaSinPotencias 
\begin{algorithm}[!ht]
\caption{Given a matrix $A \in {\mathbb{C}^{n \times n}}$ and a  small value $tol$, this algorithm calculates  calculates an order $m\in\mathbb{M}$, $m_{\min}\le m\le m_{\max}$, and the scaling factor $s$.}
\label{ms-3}
\begin{algorithmic} [1]
\State $i=\min$; $f=0$; 
\State Compute ${\alpha_0} \approx {\left\| {{A^{m_{i}}}} \right\|^{1/m_{i}}}$ from $A$  \Comment based on Algorithm 1  from~\cite{High88}
\While {$f=0$ and $i< \max$}
    \State $i=i+1$ 
    \State Compute ${\alpha} \approx {\left\| {{A^{m_{i}}}} \right\|^{1/m_{i}}}$ from $A$  \Comment based on Algorithm 1  from~\cite{High88}
    \If {$|\alpha-\alpha_0|>\alpha \cdot tol $}
        \State $\alpha_0=\alpha$ 
    \Else
        \State $f=1$
    \EndIf
\EndWhile
\State  $i=\min$; $f=0$ 
\While {$f=0$ and $i\le \max$} 
    \If {$\alpha<\Theta_{m_{i}}$}
        \State $f=1$
    \Else
        \State $i=i+1$
    \EndIf
\EndWhile
\If {$f=1$}
    \State $s=0$
\Else
    \State $i=i_{\max}$
    \State $s = \left\lceil {\max \left( {0,{{f_{s}\log }_2}({\alpha}/{\Theta_{m_{\max}}})} \right)} \right\rceil $ 
    \State $f=0$
    \State $j=i$
    \While {$f=0$}
       \State $j=j-1$
       \State $s_1 = \left\lceil {\max \left( {0,{{f_{s}\log }_2}({\alpha }/{\Theta_{m_{j}}})} \right)} \right\rceil $
       \If {$ s\ge s_1$ }
            \State $s=s_1$
            \State $i=j$
       \Else 
            \State $f=1$  
       \EndIf 
    \EndWhile   
\EndIf
\State $m=m_i$    
\end{algorithmic}
\end{algorithm}

%this algorithm calculates $m$ and $s$ by Bernouilli series \eqref{Bernoulli6}, %where only the coefficients of the even order terms have been considered.}


